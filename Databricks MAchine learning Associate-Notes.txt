
 
 Use Databricks Machine Learning and its capabilities within machine learning workflows, including:
Databricks Machine Learning (clusters, Repos, Jobs)
Databricks Runtime for Machine Learning (basics, libraries)
AutoML (classification, regression, forecasting)
Feature Store (basics)
MLflow (Tracking, Models, Model Registry)

Implement correct decisions in machine learning workflows, including:
Exploratory data analysis (summary statistics, outlier removal)
Feature engineering (missing value imputation, one-hot-encoding)
Tuning (hyperparameter basics, hyperparameter parallelization)
Evaluation and selection (cross-validation, evaluation metrics)

Implement machine learning solutions at scale using Spark ML and other tools, including:
Distributed ML Concepts
Spark ML Modeling APIs (data splitting, training, evaluation, estimators vs. transformers, pipelines)
Hyperopt
Pandas API on Spark

Pandas API on Spark provides a way for users to leverage the powerful data manipulation capabilities of Pandas on large-scale datasets using Spark's distributed computing capabilities. 
However, users should be aware of the differences between the two APIs and use them appropriately to avoid memory issues.

Convert DataFrame between Pandas, Spark & Pandas API on Spark



Just stick to the concept and the API regarding Databricks (about MLflow and the feature store and Spark ML mostly). 

Some knowledge might be needed too (overfitting vs undergitting, train test, cross validation, different kind of models…), the questions are not too technical. Basic knowledge and understanding is enough. 

 



dataframe two options:

describe: count, mean, stddev, min, max 

Example : display(df.describe())
summary: describe + interquartile range (IQR)

Example :display(df.summary())

dbutils.data.summarize(df)  -->  to see more detailed summary statistics and data plots.



Handling Null Values


Drop any records that contain nulls
Numeric:
Replace them with mean/median/zero/etc.
Categorical:
Replace them with the mode
Create a special category for null
Use techniques like ALS (Alternating Least Squares) which are designed to impute missing values
If you do ANY imputation techniques for categorical/numerical features, you MUST include an additional field specifying that field was imputed.

SparkML's Imputer  does not support imputation for categorical features.

SparkML's Imputer requires all fields be of type double  -->df.withColumn(c, col(c).cast("double")) 

The reason for this is that Imputer in Spark ML is implemented using the VectorAssembler class, which requires all input features to be of the same data type, and the default data type for numeric values in Spark ML is double. Therefore, all integer columns need to be cast to double before they can be passed to Imputer.


VectorAssembler dont have fit method since its transformer 

from pyspark.sql.functions import col

df = df.select([col(c).cast("double").alias(c) for c in df.columns])


Convert Null/None/blank value to 1.0

df.withColumn(c + "_na", when(col(c).isNull(), 1.0).otherwise(0.0))

Imputedata : ( Repalce Null value with median pre request is datatype of the column should be double)

from pyspark.ml.feature import Imputer

imputer = Imputer(strategy="median", inputCols=impute_cols, outputCols=impute_cols)

imputer_model = imputer.fit(doubles_df)
imputed_df = imputer_model.transform(doubles_df)


Transformers and Estimators are classes in machine learning used in the context of DataFrames.
 Transformers apply rule-based transformations on DataFrames without learning any parameters from the data,  The Transformer has a .transform() method,
 Estimators learn parameters from a DataFrame and produce a Transformer. the Estimator has a .fit() method.



Different type of Regression Model:


Linear models
•Linear Regression
•Ridge Regression
•Lasso Regression
•Bayesian Ridge

Non linear models
•Support Vector Machine Regression (SVR)
•Decision Tree Regression
•Random Forest Tree
•XGBoost Regressor
•Gradient Boosting Regression
•LGBM Regressor


Different type of Classfication :

Linear models

•Logistic Regression
•Naïve Bayes

Non linear models

•Support Vector Machine Classification (SVC)
•Decision Tree Classification
•Random Forest Tree
•XGBoost Classifier
•AdaBoost Classification
•k Nearest Neighbour ( kNN)




PySpark ml
model = regression_model (*input parameter )
fitted_model = model.fit (*data)  -->(* Accepts PySpark Dataframe with vectorized independent variables (can do this with separate PySpark routines)
fitted_model.transform(*data)  --> Outputs PySpark Dataframe
 



MLFlow :

Model Tracking: (end-to-end reproducibility for the machine learning training process)

mlflow.set_experiment() to set an experiment, but if you do not specify an experiment, it will automatically be scoped to this notebook

Each Notebook we have one experiementid if we start the each run we have one runid ( with mlflow.start_run(run_name="Run name") as run:)

You can query past runs programmatically  using MlflowClient object.

from mlflow.tracking import MlflowClient

client = MlflowClient()

Can find all runs for a given experiment

runs_df = mlflow.search_runs(experiment_id)

display(runs_df)

Pull the last run and look at metrics.

runs = client.search_runs(experiment_id, order_by=["attributes.start_time desc"], max_results=1)
runs[0].data.metrics


Model Registry: (reproducibility and governance for the deployment process)

centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of an MLflow Model. It provides model lineage (which MLflow Experiment and Run produced the model), model versioning, stage transitions (e.g. from staging to production), annotations (e.g. with comments, tags), and deployment management (e.g. which production jobs have requested a specific model version).


To register model in registry:

run_id = run.info.run_id
model_uri = f"runs:/{run_id}/model"
model_details = mlflow.register_model(model_uri=model_uri, name=model_name)


To get model detail from registry:

model_version_details = client.get_model_version(
    name=model_details.name,
    version=model_details.version
)



To update the Model version:

client.update_model_version(
    name=model_name,
    version=new_model_version,
    description=f"This model version is a ridge regression model with an alpha value of {alpha} that was trained in scikit-learn."
)



To Move/deploy model to Stage ( i.e None/stage/Production/Archived)
client.transition_model_version_stage(
    name=model_name,
    version=2,
    stage="Stage"
)

To delte model version:

we cant delete the model before archived ( first archived model and after that you can delete the model)
client.delete_model_version(
    name=model_name,
    version=1
)


# Import PySpark Pandas

import pyspark.pandas as ps



#create Pandas Dataframe 

df = ps.DataFrame([list])

print(df)



# Convert Pandas API on Spark to Pandas DataFrame
pdf = df.to_pandas()
print(type(pdf))

# Output
#<class 'pandas.core.frame.DataFrame'>

# Convert Pandas DataFrame to Pandas API on Spark DataFrame
psdf = ps.from_pandas(pdf)
print(type(psdf))

# Output
# <class 'pyspark.pandas.frame.DataFrame'>


# convert Pandas API on Spark Dataframe into a Spark DataFrame
sdf = df.to_spark()
# Output
class 'pyspark.sql.dataframe.DataFrame'>


# Convert a Spark Dataframe into a Pandas API on Spark Dataframe
psdf = sdf.pandas_api()
print(type(psdf))

# Output
# <class 'pyspark.pandas.frame.DataFrame'>



Pandas UDFs and Pandas Function APIs


Understand advanced scaling characteristics of classical machine learning models, including:
Distributed Linear Regression
Distributed Decision Trees
Ensembling Methods (bagging, boosting)
 
 https://www.udemy.com/course/databricks-certified-machine-learning-associate-2023/
 
 
 Reference:
 https://msdatalab.net/associate-machine-learning/
 
 
 Hands-On Training for Data Science and Machine Learning
 https://www.youtube.com/watch?v=3AdVRy1R_8s
 
 
 Model Training :
 https://learn.microsoft.com/en-gb/azure/databricks/_extras/notebooks/source/mlflow/ml-quickstart-training.html
 
 https://towardsdatascience.com/5-minutes-cheat-sheet-explaining-all-machine-learning-models-3fea1cf96f05
 https://towardsdatascience.com/machine-learning-algorithms-cheat-sheet-2f01d1d3aa37
 https://www.linkedin.com/pulse/machine-learning-algorithms-quick-summary-rahul-kulkarni/
 
 Youtube channel :
 Learn to Use Databricks for the Full ML Lifecycle -->  https://www.youtube.com/watch?v=dQD2gVPJggQ
 
 MLflow is an open source platform for managing the end-to-end machine learning lifecycle.
 Azure Databricks provides a fully managed and hosted version of MLflow integrated with enterprise security features, high availability, and other Azure Databricks
 workspace features
 
 
 Feature store :
 
 it is a centralised repository that stores curated features
 It is a data management layer that allows data scientists, machine learning engineers and data engineers to collaborate, share and discover features
 interface between the raw data and the models
  
 
 There are essentially 3 steps to doing this in Databricks:

1.Create necessary functions to compute the features. Each function should return an Apache Spark DataFrame with a unique primary key.
 The primary key can consist of one or more columns.

2.Create a feature table by instantiating a FeatureStoreClient and using create_table (Databricks Runtime 10.2 ML or above) or create_feature_table (Databricks Runtime 10.1 ML or below).

 #Feature tables are stored as Delta tables in Databricks
   2-ways:
   create_table (Databricks Runtime 10.2 ML or above) 
   create_feature_table (Databricks Runtime 10.1 ML or below)
   
   we need database to create table 
   instantiating a FeatureStoreClient and using create_table to create the feature table
   
   fs = feature_store.FeatureStoreClient()

	fs.create_table(
    name="feature_store_dbname.feature_store_tablename",
    primary_keys = ["PrimaryCol1","PrimaryCol2"],
    df = dataframename,
    description = "Description of the table details")
	
	
3.Populate the feature table using write_table.
	
fs.write_table(
  name = "feature_store_dbname.feature_store_tablename",
  df = dataframename,
  mode = "merge")
  
 Feature Store UI:
 
 Once these steps are complete,  created feature tables can be explored  under Feature Store UI using the  in the Databricks (under Machinelearning persona),
 you can track the raw data sources, notebooks, and jobs used to compute the features.



Interact with the feature tables :

--specific features from the feature table are selected for model training

from databricks.feature_store import FeatureLookup

features_table = "feature_store_dbname.feature_store_tablename"

in the FeatureLookup and if you specify no features, it will return all of them except the primary key.

features_lookups = [
    FeatureLookup( 
      table_name = features_table,
      feature_names = "Column3",
      lookup_key = ["PrimaryCol1","PrimaryCol2"],
    ),
    FeatureLookup( 
      table_name = features_table,
      feature_names = "Column4",
      lookup_key = ["PrimaryCol1","PrimaryCol2"],
    )
]


create a training dataset to train an ML model. Using the fs.create_training_set API and an object called a FeatureLookup

training_set = fs.create_training_set(
                df_train,
                feature_lookups = features_lookups,
                label = 'targetcolumns',
                exclude_columns = exclude_columns
                )



Train Model:

# Load the TrainingSet into a dataframe which can be passed into sklearn for training a model
training_df = training_set.load_df()


why we need FEATURE STORE?
when working on a project where the intention is to deploy models at scale
the Feature Store as data source: note that the feature store should also be accessible within other environments (e.g. Staging, QA, prod).
 It is feasible in Databricks. It acts as a single source of truth for all environments.

Databricks Runtime ML:

Databricks Runtime ML clusters include the most popular machine learning libraries,
such as TensorFlow, PyTorch, Keras, and XGBoost, and also include libraries required for distributed training such as Horovod. 
Using Databricks Runtime ML speeds up cluster creation and ensures that the installed library versions are compatible.

select a Databricks Runtime ML version from the Databricks Runtime Version drop-down. Both CPU and GPU-enabled ML runtimes are available.

If you select a GPU-enabled ML runtime, you are prompted to select a compatible Driver Type and Worker Type.
Incompatible instance types are grayed out in the drop-downs. GPU-enabled instance types are listed under the GPU-Accelerated label.

Databricks Runtime 9.0 ML and above, the virtualenv package manager is used to install Python packages. 
All Python packages are installed inside a single environment: /databricks/python3

Databricks Runtime 8.4 ML and below, the Conda package manager is used to install Python packages. 
All Python packages are installed inside a single environment: /databricks/python2 on clusters using Python 2 
and /databricks/python3 on clusters using Python 3. Switching (or activating) Conda environments is not supported.

Long Term Support versions are represented by an LTS qualifier (for example, 3.5 LTS). For each major release,
we declare a “canonical” feature version, for which we provide two full years of support.



MLFlow: https://mlflow.org/docs/latest/tracking.html

MLflow is an open source platform to manage the ML lifecycle,
 including experimentation, reproducibility, deployment, and a central model registry. 
 MLflow currently offers four components:
 
 MLflow Tracking : Record and query experiments: code, data, config, and results
 
 organized around the concept of runs, which are executions of some piece of data science code. Each run records the following
 Code Version
 Start & End Time of the run
 Source - Name of the file to lunch to run 
 Parameters - input parameter of the choice
 Metrics - Cap
 Artifacts - output of the file in any format 
 
 MLflow Projects: Package data science code in a format to reproduce runs on any platform
 Allow you to package ML code in a reusable, reproducible form to share with other data scientists or transfer to production.
 
 MLflow Models: Deploy machine learning models in diverse serving environments
 Allow you to manage and deploy models from a variety of ML libraries to a variety of model serving and inference platforms

 Model Registry : Store, annotate, discover, and manage models in a central repository
 Allows you to centralize a model store for managing models’ full lifecycle stage transitions: from staging to production, with capabilities for versioning and annotating.
 
 Model Serving: Allows you to host MLflow Models as REST endpoints.
 
 mlflow.sklearn.autolog --> it will automatically capture metrics and parameter 
 
 
 mlflow :https://www.youtube.com/watch?v=r0do1KVEGqM&list=PLwFaZuSL_mfou923msxLWAqxkj6Zcnt29
 
 https://github.com/krsnagaraj/dataaisummit-mlflow





 
Distributed ML Concepts
Spark ML Modeling APIs (data splitting, training, evaluation, estimators vs. transformers, pipelines)
https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbUFJckRrYWJtbzB6YnRUZDVPNzRtVEdnRWpEZ3xBQ3Jtc0ttcm5kS3hoYy1XaExsV3o2N3pEVXFBOWQza2xnVnRuQXZieWpNdHVRV21Fa3p1dmIycGszejFjQWhybVZkTmxocXJBN0NaTmpjT0RPYWw4QWQ2a19VeWg4cW1xTEM2TkdVdm5fM3R0VXRsVG5PbzdRTQ&q=https%3A%2F%2Ffiles.training.databricks.com%2Fclasses%2Fml%2Fml-on-spark.dbc&v=DqihOzZl5jM

Estimators, Transformers, Pipelines
Spark's machine learning library, MLlib, has three main abstractions:


A transformer takes a DataFrame as an input and returns a new DataFrame with one or more columns appended to it
Transformers implement a .transform() method

An estimator takes a DataFrame as an input and returns a model
Estimators implements a .fit() method.

A pipeline combines together transformers and estimators
Pipelines implement a .fit() method


Hyperopt: hyper parameter 

Hyperopt is a Python library for hyperparameter tuning

You can use Hyperopt with SparkTrials to run hyperparameter sweeps and train multiple models in parallel. 
This reduces the time required to optimize model performance.
 MLflow tracking is integrated with Hyperopt to automatically log models and parameters.
 when ever you are using mlib try to avoid SparkTrials use trials 
 below example we are using sklearn library so we have used SparkTrials

# Define the search space to explore
search_space = {
  'n_estimators': scope.int(hp.quniform('n_estimators', 20, 1000, 1)),
  'learning_rate': hp.loguniform('learning_rate', -3, 0),
  'max_depth': scope.int(hp.quniform('max_depth', 2, 5, 1)),
}
 
 def train_model(params):
  # Enable autologging on each worker
  mlflow.autolog()
  with mlflow.start_run(nested=True):
    model_hp = sklearn.ensemble.GradientBoostingClassifier(
      random_state=0,
      **params
    )
    model_hp.fit(X_train, y_train)
    predicted_probs = model_hp.predict_proba(X_test)
    # Tune based on the test AUC
    # In production settings, you could use a separate validation set instead
    roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])
    mlflow.log_metric('test_auc', roc_auc)
    
    # Set the loss to -1*auc_score so fmin maximizes the auc_score
    return {'status': STATUS_OK, 'loss': -1*roc_auc}

spark_trials = SparkTrials(
  parallelism=8
)
 
with mlflow.start_run(run_name='gb_hyperopt') as run:
  # Use hyperopt to find the parameters yielding the highest AUC
  best_params = fmin(
    fn=train_model, 
    space=search_space, 
    algo=tpe.suggest, 
    max_evals=32,
    trials=spark_trials)
	
# Sort runs by their test auc; in case of ties, use the most recent run
best_run = mlflow.search_runs(
  order_by=['metrics.test_auc DESC', 'start_time DESC'],
  max_results=10,
).iloc[0]
print('Best Run')

trials takes either a Hyperopt Trials object or SparkTrials object. Trials will run the tuning process on a single machine whereas SparkTrials will enable us to distribute the tuning process across a Spark cluster. For models created with MLlib, do not use SparkTrials however, as the model building process is automatically parallelized on the cluster. Instead, use Hyperopt's Trials class.


precision over recall

Let us say that a machine learning model is created to predict whether a certain day is a good day to launch satellites or not based on the weather.
If the model accidentally predicts that a good day to launch satellites is bad (false negative), we miss the chance to launch. This is not such a big deal.
However, if the model predicts that it is a good day, but it is actually a bad day to launch the satellites(false positive) 
then the satellites may be destroyed and the cost of damages will be in the billions.
 
recall over precsion 

Let’s say we were trying to detect if an apple was poison or not. In this case, we would want to reduce the amount of False Negatives because
 we hope to not miss any poison apples in the batch. Recall would be the best evaluation metric to use here because it measures how many poison apples we might have missed.
 We are not too concerned with mislabeling an apple as poisonous because we would rather be safe than sorry



"Precision over recall" implies that precision is more important than recall. 
Precision measures how accurate the positive predictions of the model are, and it is the ratio of true positives to the sum of true positives and false positives. 
In cases where false positives can have severe consequences, such as in the satellite launch example, precision is more critical.
Example:
Let us say that a machine learning model is created to predict whether a certain day is a good day to launch satellites or not based on the weather.
If the model accidentally predicts that a good day to launch satellites is bad (false negative), we miss the chance to launch. This is not such a big deal.
However, if the model predicts that it is a good day, but it is actually a bad day to launch the satellites(false positive) 
then the satellites may be destroyed and the cost of damages will be in the billions.

"recall over precision" implies that recall is more important than precision.
 Recall measures how many of the actual positive cases the model can correctly identify, and it is the ratio of true positives to the sum of true positives and false negatives. 
 In cases where false negatives can have severe consequences 
 Example:
 Let’s say we were trying to detect if an apple was poison or not. In this case, we would want to reduce the amount of False Negatives
 because we hope to not miss any poison apples in the batch. Recall would be the best evaluation metric to use here because it measures how many poison apples we might have missed.
 We are not too concerned with mislabeling an apple as poisonous because we would rather be safe than sorry




Normalization across instances should be done after splitting the data between training and test set, using only the data from the training set.
This is because the test set plays the role of fresh unseen data, so it's not supposed to be accessible at the training stage. Using any information coming from the test set before or during training is a potential bias in the evaluation of the performance.
When normalizing the test set, one should apply the normalization parameters previously obtained from the training set as-is. Do not recalculate them on the test set, because they would be inconsistent with the model and this would produce wrong predictions.


A data scientist is using SparkML to engineer features for an exploratory mmachine learning project. They decide they want to standardize their features using the following code block:

scaler = StandardScaler(withMean=True, inputCol="input_features", outputCol="output_features") 
scaler_model = scaler.fit(features_df) 
scaled_df=scaler_model.transform(features) 
train_df, test_df = scaled_df.randomsplit([.8, .2], seed = 42), 
Upon code review, a colleague expressed concern with the features being standardized prior to splitting the data into training set and a test set.

Which of the following changes can the data scientist make to address the concern?

The concern raised by the colleague is that standardizing features before splitting the data may result in data leakage, as the mean and standard deviation of the entire dataset are used to standardize the features. This can lead to overfitting, as the test set may be influenced by information from the training set that it should not have access to.
To address this concern, the data scientist can make the following change:
Split the data into training and test sets first, and then fit the StandardScaler on the training set only. The same scaler can then be used to transform both the training and test sets.
The modified code block would look like this:

train_df, test_df = features_df.randomSplit([0.8, 0.2], seed=42)
scaler = StandardScaler(withMean=True, inputCol="input_features", outputCol="output_features") 
scaler_model = scaler.fit(train_df) 
scaled_train_df = scaler_model.transform(train_df) 
scaled_test_df = scaler_model.transform(test_df)

This ensures that the scaler is fitted only on the training data, and not on any information from the test set, thus preventing data leakage.


.describe() function takes cols:String*(columns in df) as optional args.

.summary() function takes statistics:String*(count,mean,stddev..etc) as optional args.

scala> val df_des=Seq((1,"a"),(2,"b"),(3,"c")).toDF("id","name")
scala> df_des.describe().show(false) //without args
//Result:
//+-------+---+----+
//|summary|id |name|
//+-------+---+----+
//|count  |3  |3   |
//|mean   |2.0|null|
//|stddev |1.0|null|
//|min    |1  |a   |
//|max    |3  |c   |
//+-------+---+----+
scala> df_des.summary().show(false) //without args
//+-------+---+----+
//|summary|id |name|
//+-------+---+----+
//|count  |3  |3   |
//|mean   |2.0|null|
//|stddev |1.0|null|
//|min    |1  |a   |
//|25%    |1  |null|
//|50%    |2  |null|
//|75%    |3  |null|
//|max    |3  |c   |
//+-------+---+----+
scala> df_des.describe("id").show(false) //descibe on id column only
//+-------+---+
//|summary|id |
//+-------+---+
//|count  |3  |
//|mean   |2.0|
//|stddev |1.0|
//|min    |1  |
//|max    |3  |
//+-------+---+
scala> df_des.summary("count").show(false) //get count summary only
//+-------+---+----+
//|summary|id |name|
//+-------+---+----+
//|count  |3  |3   |
//+-------+---+----+



The machine learning engineer needs to ensure that the column "features" in the DataFrame train_df contains a vector of numerical features that 
can be used as input to the LinearRegression model. 
If the "features" column does not exist or is not in the correct format, the engineer needs to create or transform it using Spark's feature engineering capabilities.

Here are some possible steps that the engineer may need to take:

1.Define a VectorAssembler to combine multiple numerical features into a single vector column, and apply it to the DataFrame train_df to create a new DataFrame with the "features" column:


from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=[<list of feature column names>], outputCol="features")
train_df = assembler.transform(train_df)

2.Check the schema of the new DataFrame train_df to make sure it has the "features" column of the correct format.

3.Use the same code block as before to train the LinearRegression model on the DataFrame train_df:

lr = LinearRegression(featuresCol="features", labelCol="price")
lr_model = lr.fit(train_df)

Optionally, evaluate the performance of the model on a separate validation set or by cross-validation to check for overfitting or underfitting.

Note that the engineer may also want to preprocess or normalize the numerical features before feeding them to the LinearRegression model, 
depending on the nature and distribution of the data. SparkML provides several feature transformers for this purpose, such as StandardScaler, MinMaxScaler



A machine learning engineer is converting a decision tree from skcit-learn to Spark ML. 
They notice that they are receiving different results despite all of their data and manually specified hyperparameter being identical.

Which of the following describes a reason that the single-node sklearn decision tree and spark ML decision Tree differ?

here could be several reasons why the single-node scikit-learn decision tree and Spark ML decision tree are producing different results, such as:

Randomness: Decision tree algorithms, like many other machine learning algorithms, often involve some randomization. In particular, some variants of decision trees, like Random Forests and Gradient Boosted Trees, involve bootstrapping and/or random feature selection. If the random seed or random state used in the two implementations is not the same, the resulting trees will be different.

Implementation details: The specific details of how the decision tree algorithm is implemented in scikit-learn and Spark ML may differ, leading to slightly different results. For example, the splitting criterion, stopping criteria, and pruning methods may be different in the two implementations.

Hyperparameters: The hyperparameters used to configure the decision tree algorithm may be different in the two implementations. Even small differences in hyperparameter values can lead to significant differences in the resulting tree.

Data preprocessing: The preprocessing steps used to prepare the data for the decision tree algorithm may be different in the two implementations. For example, the two implementations may use different methods for handling missing values or encoding categorical variables, which can affect the resulting tree.

Overall, the differences could stem from a variety of factors, and it is important to carefully examine each implementation to identify the root cause of the differences.

However, for decision trees, and in particular, random forests, we should not OHE our variables:

"One-hot encoding categorical variables with high cardinality can cause inefficiency in tree-based methods. Continuous variables will be given more importance than the dummy variables by the algorithm, which will obscure the order of feature importance and can result in poorer performance."

Blog:
https://towardsdatascience.com/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769#:~:text=One%2Dhot%20encoding%20categorical%20variables,importance%20resulting%20in%20poorer%20performance

Scale Invariant:
With decision trees, the scale of the features does not matter. For example, it will split 1/3 of the data if that split point is 100 or if it is normalized to be .33. The only thing that matters is how many data points fall left and right of that split point - not the absolute value of the split point.



Spark ML decision trees test more split candidates in the splitting algorithm?

Spark ML decision trees and scikit-learn decision trees may have different default settings for the number of split candidates considered during the splitting algorithm. In particular, Spark ML decision trees may test more split candidates than scikit-learn decision trees by default.

In scikit-learn, the default value for the maximum number of features used in each split is the square root of the total number of features. This means that, by default, scikit-learn decision trees will consider only a subset of the available features at each split. This is known as "feature subsampling" or "random feature selection."

In contrast, Spark ML decision trees use a different default strategy called "best" split selection, which tests all possible split candidates across all features. This means that Spark ML decision trees will generally consider more split candidates than scikit-learn decision trees. However, this can lead to slower training times and a greater risk of overfitting, especially on high-dimensional datasets.

It's important to note that both scikit-learn and Spark ML allow users to specify their own hyperparameters for the splitting algorithm, including the maximum number of features used in each split and the number of split candidates tested. Therefore, it's possible to configure both implementations to use the same settings if desired.


In summary, "fit" is used for training the model on the training dataset, and "transform" is used for making predictions on a new dataset using the trained model.


cross-validation process:

To keep the final model separate from the cross-validation process and to use a separate test set to evaluate the final model's performance, you can follow these general steps:

Split the original dataset into three subsets: training, validation, and test. The training set is used to fit the model, the validation set is used for hyperparameter tuning via cross-validation, and the test set is used to evaluate the final performance of the selected model.

Build a pipeline that includes all the feature engineering stages and the model. However, instead of using this pipeline directly in the cross-validator, create a separate instance of the model and fit it on the training data.

After fitting the model on the training data, use the validation data to perform hyperparameter tuning via cross-validation. This involves creating a new instance of the pipeline and passing it as the estimator parameter to a cross-validator object, along with a parameter grid to specify the hyperparameters to tune.

After hyperparameter tuning, select the best model based on its performance on the validation data, and use this selected model to make predictions on the test data.

Here's an example code snippet that illustrates these steps:

python
Copy code
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

# Split the data into training, validation, and test sets
training_data, validation_data, test_data = data.randomSplit([0.6, 0.2, 0.2], seed=42)

# Build a pipeline that includes all the feature engineering stages and the model
pipeline = Pipeline(stages=[... feature engineering stages ..., LogisticRegression()])

# Fit the pipeline on the training data to obtain a separate instance of the model
model = pipeline.fit(training_data)

# Use a cross-validator to perform hyperparameter tuning on the validation data
param_grid = ParamGridBuilder()... define hyperparameters to tune ... .build()
cv = CrossValidator(estimator=pipeline,
                    evaluator=BinaryClassificationEvaluator(),
                    estimatorParamMaps=param_grid,
                    numFolds=3)
cv_model = cv.fit(validation_data)

# Select the best model based on its performance on the validation data
best_model = cv_model.bestModel

# Use the selected model to make predictions on the test data
predictions = best_model.transform(test_data)

# Evaluate the performance of the selected model on the test data
evaluator = BinaryClassificationEvaluator()
performance = evaluator.evaluate(predictions)


Crossvalidation first split the data and then apply the pipeline for each split lets take numfold =3 
first fold split data 2(training):1(testing) , apply the pipeline(feature engineering, model) on the 2(training data) and evaluate  it in the 1(testing data) and measure score
similar process will do for other two fold and average score

or

first split the data and  pre-proecess data and  pass preprocess data to crossvalidation and apply only  just model 

cross-validation outside  pipeline :
cv = CrossValidator(estimator=pipeline, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)

it is a good choice when you're interested in evaluating the overall performance of the model using cross-validation. This method is simple and easy to understand, and it allows you to easily modify the pipeline by changing the steps or hyperparameters.

cross-validation inside the pipeline:

it is a good choice when you're interested in tuning the hyperparameters of the model using cross-validation. This method allows you to perform a grid search over a range of hyperparameters and cross-validate the performance of each combination of hyperparameters. This method can be more computationally expensive than the first method, but it can lead to better hyperparameter tuning and ultimately better model performance.

How to install external libraries in Databricks

1. Direcly install in workspace

%sh
pip install packagename== versionno

2.include in the library in the interactive cluster

open cluster under libraries section and install new 

select library source :Pypi/dbfs/maven/CRAN/upload

3.create library notebook and use it in job cluster 

download the whell files in your local machine and upload into databricks 

create --> library --> choose python wheel which we downloaded 

it will create new libary in the workspace

(unselect option --> Install automatically on all cluster)

create job and dependent libraries ( please select the libraries in the workspace)


Data Cleaning:
-----------
def replace(column, value):
    return when(column!=value,column).otherwise(lit(None))
We made a function to replace a column value and replace with the None.

data = data.withColumn("Market Categories ", replace(col("Market Category"),"N/A"))


Count all the null values:

from pyspark.sql.functions import when,lit,count,isnan,col
data.select([count(when(isnan(c)|col(c).isNull(),c)).alias(c) for c in data.columns]).show()


Drop the NaN values :

#deleting the column Market Category
data = data.drop("Market Category")
# deleting the all null values 
data = data.na.drop()


List of Estimators and Transformers in PySpark's ML library:


Estimators:

pyspark.ml.classification.LogisticRegression
pyspark.ml.classification.RandomForestClassifier
pyspark.ml.regression.LinearRegression
pyspark.ml.clustering.KMeans
pyspark.ml.feature.VectorAssembler
pyspark.ml.feature.StringIndexer
pyspark.ml.feature.OneHotEncoder
pyspark.ml.evaluation.RegressionEvaluator

Transformers:

pyspark.ml.feature.StandardScaler
pyspark.ml.feature.PCA
pyspark.ml.feature.Tokenizer
pyspark.ml.feature.StopWordsRemover
pyspark.ml.feature.HashingTF
pyspark.ml.feature.IDF
pyspark.ml.feature.Word2Vec
pyspark.ml.feature.VectorIndexer

----correct way of using pipeline and  cv

from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import VectorAssembler, StandardScaler
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

# Load data
data = spark.read.format("csv").option("header", "true").load("data.csv")

# Define the feature vector assembler
assembler = VectorAssembler(inputCols=["feature1", "feature2", "feature3"], outputCol="features")

# Define the standard scaler transformer
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")

# Define the logistic regression estimator
lr = LogisticRegression(featuresCol="scaledFeatures", labelCol="label", fitIntercept=False)

# Define the pipeline
pipeline = Pipeline(stages=[assembler, scaler, lr])

# Split data into training and test sets
(trainingData, testData) = data.randomSplit([0.8, 0.2], seed=42)

# Define the cross-validator
cv = CrossValidator(estimator=pipeline,
                    estimatorParamMaps=ParamGridBuilder().build(),
                    evaluator=BinaryClassificationEvaluator(),
                    numFolds=5,
                    seed=42)

# Fit the cross-validator to the training data
cvModel = cv.fit(trainingData)

# Evaluate the performance on the test data
predictions = cvModel.transform(testData)
evaluator = BinaryClassificationEvaluator()
auc = evaluator.evaluate(predictions)
print("AUC on test data: {}".format(auc))

We set fitIntercept=False for the logistic regression estimator to avoid fitting the intercept separately for each fold in the cross-validation process.
we split the data into training and test sets, and define a cross-validator with appropriate settings. We set seed=42 for both the randomSplit and CrossValidator methods to ensure reproducibility of the results.



Parameter: In machine learning, a parameter refers to a variable that is learned by the model during the training process. For example, in linear regression, the parameters are the intercept and the coefficients. Once the model is trained, the parameters are fixed and used to make predictions on new data.

Hyperparameter: A hyperparameter is a setting or configuration that is set by the user before the training process begins. Hyperparameters are not learned by the model, but rather are chosen by the user to optimize the performance of the model. Examples of hyperparameters include learning rate, regularization strength, and number of hidden layers in a neural network.

Hyperopt: Hyperopt is a Python library for hyperparameter optimization. It uses a Bayesian optimization approach to find the optimal values for the hyperparameters of a machine learning model. Hyperopt works by searching the space of possible hyperparameters and iteratively updating a probability distribution over the space based on the performance of the model. By using Hyperopt, users can efficiently search for the best hyperparameters for their model without the need for manual tuning.

In summary, parameters are variables learned by the model, hyperparameters are settings or configurations set by the user, and Hyperopt is a library for optimizing the hyperparameters of machine learning models.



ParamGridBuilder: ParamGridBuilder is a class in Apache Spark's MLlib library that allows users to specify a grid of hyperparameters to search over during model selection. The class takes in hyperparameters as input and generates a grid of all possible combinations of hyperparameters. The user can then use this grid to train and evaluate multiple models with different hyperparameter settings.

ParamGridBuilder: The output of ParamGridBuilder is a grid of hyperparameter settings that the user defines. This grid is a list of dictionaries where each dictionary contains a set of hyperparameters and their corresponding values. For example, if the user specifies two hyperparameters maxDepth and numTrees with two possible values for each, the resulting grid would contain four sets of hyperparameters: {'maxDepth': 2, 'numTrees': 10}, {'maxDepth': 2, 'numTrees': 20}, {'maxDepth': 4, 'numTrees': 10}, and {'maxDepth': 4, 'numTrees': 20}. The user can then use this grid to train and evaluate multiple models with different hyperparameter settings.



ParamGridBuilder	--> A grid of hyperparameters to explore and evaluate



Hyperopt: Hyperopt is a Python library for hyperparameter optimization that uses a Bayesian optimization approach to efficiently search the space of hyperparameters. Unlike ParamGridBuilder, Hyperopt does not require users to pre-define a grid of hyperparameters. Instead, it starts with an initial set of hyperparameters and iteratively evaluates the model performance to find the best hyperparameters. Hyperopt also supports a wider range of hyperparameters than ParamGridBuilder, including both discrete and continuous hyperparameters.


Hyperopt: The output of Hyperopt is the best hyperparameter settings found by the optimization algorithm. Specifically, Hyperopt returns a dictionary containing the best set of hyperparameters and their corresponding values based on the evaluation metric specified by the user. The user can then use these hyperparameters to train and evaluate the final model.

Hyperopt -->	The best set of hyperparameters found by the optimization algorithm


pipeline with in CV & cv within pipeline:

Pipeline within CV can be a good approach when we want to evaluate the performance of the entire pipeline on different folds of the dataset, providing a more accurate estimate of the model's performance. This approach can be particularly useful when we have complex data preprocessing steps that may affect the performance of the machine learning model.

On the other hand, CV within pipeline can be a good approach when we want to tune the hyperparameters of the machine learning model using the CV technique. This approach can be particularly useful when we have a simple data preprocessing step and we want to focus on optimizing the hyperparameters of the machine learning model.

from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.datasets import load_iris

# Load the iris dataset
iris = load_iris()

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)

# Define the pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('clf', SVC())
])

# Define the hyperparameters to tune using grid search
param_grid = {
    'clf__C': [0.1, 1, 10],
    'clf__kernel': ['linear', 'rbf']
}

# Perform pipeline within CV
cv = 5
scores = cross_val_score(pipeline, X_train, y_train, cv=cv)

# Print the pipeline within CV scores
print("Pipeline within CV scores:", scores)
print("Mean score:", scores.mean())

# Perform CV within pipeline
grid_search = GridSearchCV(pipeline, param_grid, cv=cv)
grid_search.fit(X_train, y_train)

# Print the CV within pipeline results
print("Best parameters:", grid_search.best_params_)
print("Best score:", grid_search.best_score_)
print("Test score:", grid_search.score(X_test, y_test))

RMLSE vs RMSE :

Case 1:

Y = 100

X = 90

Calculated RMLSE: 0.1053

Calculated RMSE: 10

At first glance, nothing looks flashy. Let’s consider another example.

Case 2:

Consider

Y = 10000

X = 9000

Calculated RMSLE: 0.1053

Calculated RMSE : 1000

Surprised?

These two examples perfectly support the argument of the relative error which we mentioned above, RMSLE metric only considers the relative error between and the Predicted and the actual value and the scale of the error is not significant. On the other hand, RMSE value Increases in magnitude if the scale of error increases.

This is especially useful for business cases where the underestimation of the target variable is not acceptable but overestimation can be tolerated.


TrainValidationSplit only evaluates each combination of parameters once, as opposed to k times in the case of CrossValidator. It is therefore less expensive, but will not produce as reliable results when the training dataset is not sufficiently large.

Cross-validation over a grid of parameters is expensive. E.g., in the example below, the parameter grid has 3 values for hashingTF.numFeatures and 2 values for lr.regParam, and CrossValidator uses 2 folds. This multiplies out to (3×2)×2=12
 different models being trained
 
 
 
 Model
An MLflow Model is created from an experiment or run that is logged with one of the model flavor’s mlflow.<model_flavor>.log_model() methods. Once logged, this model can then be registered with the Model Registry.

Registered Model
An MLflow Model can be registered with the Model Registry. A registered model has a unique name, contains versions, associated transitional stages, model lineage, and other metadata.

Model Version
Each registered model can have one or many versions. When a new model is added to the Model Registry, it is added as version 1. Each new model registered to the same model name increments the version number.

Model Stage
Each distinct model version can be assigned one stage at any given time. MLflow provides predefined stages for common use-cases such as Staging, Production or Archived. You can transition a model version from one stage to another stage.

Annotations and Descriptions
You can annotate the top-level model and each version individually using Markdown, including description and any relevant information useful for the team such as algorithm descriptions, dataset employed or methodology.


Reference:https://mlflow.org/docs/1.8.0/model-registry.html


Log the model using mlflow.spark.log_model(). This creates a new directory within the run's artifacts where the model will be saved. For example:


import mlflow.spark

mlflow.spark.log_model(spark_model=trained_model, artifact_path="model")
  mlflow.log_param("num_features", num_features)
    mlflow.log_metric("auc", auc)



Exploratory data analysis on Databricks: Tools and techniques


Visualization tasks in Databricks SQL

After running a query, in the Results panel, click + and then select Visualization.

In the Visualization Type drop-down, choose Bar.

Enter a visualization name 

Review the visualization properties.

Click Save.



Note that the default metric for the BinaryClassificationEvaluator is areaUnderROC

The evaluator accepts two kinds of metrics - areaUnderROC and areaUnderPR. Set it to areaUnderPR by using evaluator.setMetricName("areaUnderPR").

crossvalidator has to choose bestModel
----------------------------

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
paramGrid = (ParamGridBuilder()
             .addGrid(dt.maxDepth, [1, 2, 6, 10])
             .addGrid(dt.maxBins, [20, 40, 80])
             .build())
			 
cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)
 
# Run cross validations
cvModel = cv.fit(trainingData)

print("numNodes = ", cvModel.bestModel.numNodes)
print("depth = ", cvModel.bestModel.depth)



 wrappedModel = SklearnModelWrapper(model)  --> SklearnModelWrapper is class 
  # Log the model with a signature that defines the schema of the model's inputs and outputs. 
  # When the model is deployed, this signature will be used to validate inputs.
  signature = infer_signature(X_train, wrappedModel.predict(None, X_train))
  
  
 Feature Store retrival:
-------------------------- 
 To define a list of features to include from the Feature Store table in the FeatureLookup constructor, you can simply pass a list of the feature names as the third argument. Here's an example code snippet that includes three specific features from the "features" table:
 
lookups = [FeatureLookup("features", "customer_id", ["feature1", "feature2", "feature3"])]
training_set = fs.create_training_set(training_df, lookups, exclude_columns=["customer_id"])

In this code, the FeatureLookup constructor includes a list of features to retrieve from the "features" table, which includes "feature1", "feature2", and "feature3". The exclude_columns parameter is used to exclude the "customer_id" column from training_df in  the resulting training set.

If you want to include all of the features in the Feature Store table, you can omit the list of feature names from the FeatureLookup constructor altogether. In that case, all features in the table will be retrieved for the join key(s) specified in the constructor.
  The exclude_columns argument is used to exclude the "customer_id" column from the training set (training_df)
  
 
benefit of using vectorized pandas UDFs instead of the standard PySpark UDFs? 
----------------------------------------


Pandas UDFs, also known as scalar UDFs, operate on a single row or group of rows at a time. These UDFs are executed on each row or group independently and sequentially, which can result in slower processing times for large datasets. Pandas UDFs are written in Python and can utilize the full Pandas library to perform data transformations.

On the other hand, vectorized pandas UDFs, also known as Grouped Map Pandas UDFs, operate on a group of rows at a time. These UDFs are vectorized, meaning that they take advantage of Pandas' fast and efficient processing of entire arrays of data at once, resulting in significantly faster processing times for large datasets. Vectorized pandas UDFs are written in Python, but use optimized C++ code under the hood, which makes them faster than scalar UDFs.




Vectorized pandas UDFs allow you to use the full power of the pandas API inside the function, which can be a significant advantage for data processing tasks.

Since the vectorized pandas UDFs operate on batches of data, the input and output of the function are pandas DataFrame objects, which means that you can use all of the pandas API functions to manipulate and analyze the data.
Pandas UDFs leverage the power of vectorized operations in NumPy and pandas to process large amounts of data in a single operation, which can be much faster than processing the data row by row as in standard PySpark UDFs

data being processed is too large to fit into the memory of a single executor node, it may be necessary to use standard PySpark UDFs instead of pandas UDFs to ensure that the data can be processed efficiently and without running out of memory.



predict(*[spark_df[col] for col in spark_df.columns])

is equivalent to calling predict(spark_df[col1], spark_df[col2], ..., spark_df[colN]), where col1 through colN are the columns of spark_df.


Pandas Scalar Iterator UDF
If your model is very large, then there is high overhead for the Pandas UDF to repeatedly load the same model for every batch in the same Python worker process. In Spark 3.0, Pandas UDFs can accept an iterator of pandas.Series or pandas.DataFrame so that you can load the model only once instead of loading it for every series in the iterator.

This way the cost of any set-up needed will be incurred fewer times. When the number of records you’re working with is greater than spark.conf.get('spark.sql.execution.arrow.maxRecordsPerBatch'), which is 10,000 by default, you should see speed ups over a pandas scalar UDF because it iterates through batches of pd.Series.

It has the general syntax of:

@pandas_udf(...)
def predict(iterator):
    model = ... # load model
    for features in iterator:
        yield model.predict(features)

Scalar Pandas UDFs
-------------------
Scalar Pandas UDFs are used for vectorizing scalar operations. To define a scalar Pandas UDF, simply use @pandas_udf to annotate a Python function that takes in pandas.Series as arguments and returns another pandas


from pyspark.sql.functions import pandas_udf, PandasUDFType

# Use pandas_udf to define a Pandas UDF
@pandas_udf('double', PandasUDFType.SCALAR)
# Input/output are both a pandas.Series of doubles

def pandas_plus_one(v):
    return v + 1

df.withColumn('v2', pandas_plus_one(df.v))

Grouped Map Pandas UDFs
------------------
Python users are fairly familiar with the split-apply-combine pattern in data analysis. Grouped map Pandas UDFs are designed for this scenario, and they operate on all the data for some group, e.g., "for each date, apply this operation".


Input of the user-defined function:
Scalar: pandas.Series
Grouped map: pandas.DataFrame

Output of the user-defined function:
Scalar: pandas.Series
Grouped map: pandas.DataFrame


pandas function APIs in PySpark, which enable users to apply Python native functions that take and output pandas instances directly to a PySpark DataFrame. There are three types of pandas function APIs: grouped map, map, and cogrouped map.

The grouped map API allows users to split data into groups using DataFrame.groupBy and apply a function on each group using groupBy().applyInPandas(). The output is then combined into a new DataFrame.

Example:
df = spark.createDataFrame(
    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],
    ("id", "v"))

def subtract_mean(pdf):
    # pdf is a pandas.DataFrame
    v = pdf.v
    return pdf.assign(v=v - v.mean())

df.groupby("id").applyInPandas(subtract_mean, schema="id long, v double").show()

o/p:
# +---+----+
# | id|   v|
# +---+----+
# |  1|-0.5|
# |  1| 0.5|
# |  2|-3.0|
# |  2|-1.0|
# |  2| 4.0|
# +---+----+

The map API is used to transform an iterator of pandas.DataFrame to another iterator of pandas.DataFrame that represents the current PySpark DataFrame using DataFrame.mapInPandas().

Example :
df = spark.createDataFrame([(1, 21), (2, 30)], ("id", "age"))

def filter_func(iterator):
    for pdf in iterator:
        yield pdf[pdf.id == 1]

df.mapInPandas(filter_func, schema=df.schema).show()

o/p:

# +---+---+
# | id|age|
# +---+---+
# |  1| 21|
# +---+---+

The cogrouped map API enables users to cogroup two PySpark DataFrames by a common key and apply a Python function to each cogroup using DataFrame.groupby().cogroup().applyInPandas(). The output is then combined into a new PySpark DataFrame.

Example:
import pandas as pd

df1 = spark.createDataFrame(
    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],
    ("time", "id", "v1"))

df2 = spark.createDataFrame(
    [(20000101, 1, "x"), (20000101, 2, "y")],
    ("time", "id", "v2"))

def asof_join(l, r):
    return pd.merge_asof(l, r, on="time", by="id")

df1.groupby("id").cogroup(df2.groupby("id")).applyInPandas(
    asof_join, schema="time int, id int, v1 double, v2 string").show()
	
o/p:
	
# +--------+---+---+---+
# |    time| id| v1| v2|
# +--------+---+---+---+
# |20000101|  1|1.0|  x|
# |20000102|  1|3.0|  x|
# |20000101|  2|2.0|  y|
# |20000102|  2|4.0|  y|
# +--------+---+---+---+

df.apply --> one column or row apply at a time with function 
df['column'].apply () --> at a time one element in the particular column apply function 
df['column'].map () --> each element which column specified map function
df.applymap --> apply on whole data frame ( each element one by one )



score model using feature store in databricks
-------------------------

# Import required libraries
from databricks.feature_store import FeatureStoreClient
from pyspark.ml import PipelineModel

# Initialize a feature store client
feature_store = FeatureStoreClient()

# Load the model from the saved location
model_path = "/path/to/saved/model"
model = PipelineModel.load(model_path)

# Retrieve the feature table from the feature store
feature_table = feature_store.get_feature_table("feature_table_name")

# Retrieve the latest version of the feature table
latest_version = feature_table.latest_version

# Specify the input data for the model
input_data = feature_store.get_online_features(
    feature_table=feature_table,
    features=["feature_1", "feature_2", "feature_3"],
    version=latest_version
)

# Score the model using the input data
output_data = model.transform(input_data)

# Display the output data
display(output_data)



identify scenario single node cluster over standard cluster in  databricks 
---------------------------------
Scenario						Single Node Cluster						Standard Cluster
Development and Testing			Sufficient and less expensive			May be overkill
Low-volume Batch Processing		Sufficient for small jobs				May be necessary for larger jobs
Ad-hoc Interactive Analysis		Sufficient for small datasets			May be necessary for larger datasets
Large-scale Data Processing		May be insufficient						Necessary for distributing workload
High-concurrency Workloads		May not support multiple users			Necessary for supporting multiple users
Resilience and Fault Tolerance	May not provide sufficient redundancy	Provides redundancy and fault tolerance


selection algorithm 
------------

There are several algorithms used for hyperparameter selection in machine learning. Here are some of the most commonly used selection algorithms:

Grid search: In this algorithm, the hyperparameters are defined as a set of discrete values, and all possible combinations of the hyperparameters are evaluated exhaustively. While this approach is easy to implement, it can be computationally expensive for large search spaces.

Random search: This algorithm randomly samples hyperparameters from a defined search space. This approach can be more efficient than grid search, especially when searching through high-dimensional search spaces.

Bayesian optimization: This algorithm uses a probabilistic model of the objective function to iteratively update the hyperparameters to be evaluated. This approach can be more efficient than grid or random search, especially for high-dimensional search spaces, but requires more computational resources.

Evolutionary algorithms: These algorithms are inspired by natural selection and involve generating a population of hyperparameter configurations, evaluating their fitness, and selecting the fittest individuals to reproduce and generate the next generation of configurations. This approach can be useful for exploring large and complex search spaces.

Gradient-based optimization: These algorithms involve optimizing the hyperparameters using gradient-based methods, such as gradient descent or L-BFGS. This approach can be efficient for small search spaces, but may not be practical for high-dimensional search spaces.

Ensemble-based methods: These algorithms involve training multiple models with different hyperparameter configurations and combining their predictions to make final predictions. This approach can be useful for reducing the impact of hyperparameter selection on model performance.


selection algorithms, such as Bayesian optimization, can be more challenging to parallelize because they involve iterative optimization of a complex objective function. In this case, parallelization can be achieved by running multiple independent optimization processes in parallel, or by using techniques such as multi-armed bandit algorithms to balance exploration and exploitation of the hyperparameter space.




Spark DataFrames and the Pandas API: Understanding Immutability
-----------------------------------

As a data scientist or engineer working with Spark, you may be familiar with the Pandas API that allows you to use the familiar syntax and functions of Pandas with Spark DataFrames. However, it's important to understand the differences in behavior between Pandas and Spark, particularly when it comes to immutability.

By default, Spark DataFrames are immutable, which means that operations on them return new DataFrames instead of modifying the original one.



 This is different from Pandas, where you can modify the original DataFrame in place using inplace=True or .loc.

While inplace=True and .loc can be useful for memory efficiency or specific data manipulation tasks, they can also introduce risks and unexpected behavior in the context of Spark DataFrames. 

Therefore, it's generally recommended to follow the functional programming paradigm and avoid modifying data in place.

By understanding the immutability behavior of Spark DataFrames and the Pandas API, you can ensure that your data processing and analysis workflows are reliable, scalable, and optimized for the Spark platform.

-------------------------

feature				Koalas																			Pandas API
Library				Koalas is a third-party library built on top of Apache Spark.					The Pandas API is a built-in feature of Apache Spark.
Syntax				Koalas provides a Pandas-like syntax and API for working with Spark DataFrames.	The Pandas API provides a Pandas-like syntax and API for working with Spark DataFrames.
Performance			Koalas may have some overhead due to the additional layer of abstraction between Pandas and Spark.	The Pandas API can offer better performance since it    																													doesnot have the additional overhead of a separate library.
Learning Curve		Koalas can be easier for users who are already familiar with Pandas and want to leverage their existing knowledge and code.	The Pandas API may have a steeper learning curve for users who are not already familiar with Pandas.
Additional Features	Koalas may provide some additional features or functionality on top of the standard DataFrame format, such as support for some Spark-specific data types or functions.	The Pandas API on Spark may offer some additional features or optimizations for working with distributed data, such as partitioning and caching.



One Hot Encoder
Here, we are going to One Hot Encode (OHE) our categorical variables. Spark doesn't have a dummies function, and OHE is a two step process. First, we need to use StringIndexer to map a string column of labels to an ML column of label indices.

Then, we can apply the OneHotEncoder to the output of the StringIndexer.


Tree Based Algorithm:
--------------------

The "maxBins" parameter is a mechanism in Spark MLlib's Decision Tree implementation that controls the maximum number of bins used for discretizing continuous features. Discretization is a process of transforming continuous features into categorical features by dividing them into a set of bins or intervals. The "maxBins" parameter determines the maximum number of bins that can be used to discretize continuous features.

In Spark MLlib, the default value of "maxBins" is set to 32. However, you can adjust this parameter based on the nature of your data and the complexity of the problem you're trying to solve. Increasing the value of "maxBins" can improve the accuracy of the model, but it can also increase the computational cost and memory requirements of training the model.

If your dataset contains many continuous features, you may need to increase the value of "maxBins" to obtain better results. On the other hand, if your dataset has a small number of continuous features, you may be able to reduce the value of "maxBins" to save computational resources.

In summary, "maxBins" is a mechanism in Spark MLlib's Decision Tree implementation that controls the maximum number of bins used for discretizing continuous features. You can adjust this parameter based on the nature of your data and the complexity of your problem to improve the accuracy of the model and optimize computational resources.


Scale Invariant
--------------------
With decision trees, the scale of the features does not matter. For example, it will split 1/3 of the data if that split point is 100 or if it is normalized to be .33. The only thing that matters is how many data points fall left and right of that split point - not the absolute value of the split point.

This is not true for linear regression, and the default in Spark is to standardize first. Think about it: If you measure shoe sizes in American vs European sizing, the corresponding weight of those features will be very different even those those measures represent the same thing: the size of a person's foot!


Linear regression Algorithm:
---------------------

from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler

vec_assembler = VectorAssembler(inputCols=["bedrooms"], outputCol="features")
#vec_assembler_m = vec_assembler.fit (train_df)
vec_train_df = vec_assembler.transform(train_df)


lr = LinearRegression(featuresCol="features", labelCol="price")
lr_model = lr.fit(vec_train_df)


vec_test_df = vec_assembler.transform(test_df)

pred_df = lr_model.transform(vec_test_df)

pred_df.select("bedrooms", "features", "price", "prediction").show()

from pyspark.ml.evaluation import RegressionEvaluator

regression_evaluator = RegressionEvaluator(predictionCol="prediction", labelCol="price", metricName="rmse")

rmse = regression_evaluator.evaluate(pred_df)
print(f"RMSE is {rmse}")

 
r2 = regression_evaluator.setMetricName("r2").evaluate(pred_df)
print(f"RMSE is {rmse}")
print(f"R2 is {r2}")

Assumption of Linear regression
----------------------

Linear regression is a statistical method used to study the relationship between a dependent variable and one or more independent variables. The assumptions of the linear regression model are as follows:

Linearity: The relationship between the independent and dependent variables is linear. This means that the change in the dependent variable is proportional to the change in the independent variable.

Independence: The observations are independent of each other. This means that the value of the dependent variable for one observation does not affect the value of the dependent variable for another observation.

Homoscedasticity: The variance of the errors is constant across all levels of the independent variable. This means that the errors have the same variance for all values of the independent variable.

Normality: The errors are normally distributed. This means that the distribution of errors should be a normal distribution.

No multicollinearity: There is no high correlation between the independent variables. This means that the independent variables should be uncorrelated or only weakly correlated with each other.

Violations of these assumptions can lead to biased or inefficient estimates of the regression coefficients, and may also result in invalid inferences and predictions. Therefore, it is important to check these assumptions before using linear regression for any statistical analysis.


Call mlflow.autolog() before your training code. This will enable autologging for each supported library you have installed as soon as you import it.

Enable autologging at the workspace level from the admin console

Use library-specific autolog calls for each library you use in your code. (e.g. mlflow.spark.autolog())



There are many algorithms that can handle missing values in data, some of which are:

Decision Trees: Decision trees can handle missing values by splitting data based on available features at each node of the tree. This allows them to handle missing values without any explicit imputation.

Random Forests: Random forests are an ensemble learning method that combines multiple decision trees to improve performance. They can handle missing values by averaging the predictions of multiple trees that were trained on different subsets of the data.

K-Nearest Neighbors: KNN can handle missing values by finding the k-nearest neighbors based on the available features and then imputing the missing values using the values from the nearest neighbors.

Multiple Imputation by Chained Equations (MICE): MICE is an imputation technique that uses a series of regression models to impute missing values. It creates multiple imputed datasets and then combines them to create a final result.

Gradient Boosted Trees: Gradient Boosted Trees are another ensemble learning method that can handle missing values by splitting the data based on available features and learning from the residual errors.

Support Vector Machines (SVM): SVM can handle missing values by finding the hyperplane that maximizes the margin between different classes. The missing values are imputed using the mean or median of the available values.

Naive Bayes: Naive Bayes can handle missing values by ignoring the missing values during the calculation of the conditional probabilities.

These are just some of the algorithms that can handle missing values. The choice of algorithm will depend on the specific problem, the type and amount of missing data, and the available computational resources